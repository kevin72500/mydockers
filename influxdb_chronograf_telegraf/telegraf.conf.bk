[global_tags]

# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "10s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## For failed writes, telegraf will cache metric_buffer_limit metrics for each
  ## output, and will flush this buffer on a successful write. Oldest metrics
  ## are dropped first when this buffer fills.
  ## This buffer only fills when writes fail to output plugin(s).
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Default flushing interval for all outputs. You shouldn't set this below
  ## interval. Maximum flush_interval will be flush_interval + flush_jitter
  flush_interval = "10s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s.
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  precision = ""

  ## Logging configuration:
  ## Run telegraf with debug log messages.
  debug = false
  ## Run telegraf in quiet mode (error log messages only).
  quiet = false
  ## Specify the log file name. The empty string means to log to stderr.
  logfile = ""

  ## Override default hostname, if empty use os.Hostname()
  hostname = "webserver111117"
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = false

  ###############################################################################
  #                            OUTPUT PLUGINS                                   #
  ###############################################################################

  # Configuration for sending metrics to InfluxDB
  [[outputs.influxdb]]
    ## The full HTTP or UDP URL for your InfluxDB instance.
    ##
    ## Multiple URLs can be specified for a single cluster, only ONE of the
    ## urls will be written to each interval.
    # urls = ["unix:///var/run/influxdb.sock"]
    # urls = ["udp://127.0.0.1:8089"]
    urls = ["http://192.168.108.99:8086"]

    ## The target database for metrics; will be created as needed.
    database = "telegraf"

    ## If true, no CREATE DATABASE queries will be sent.  Set to true when using
    ## Telegraf with a user without permissions to create databases or when the
    ## database already exists.
    # skip_database_creation = false

    ## Name of existing retention policy to write to.  Empty string writes to
    ## the default retention policy.
    # retention_policy = ""

    ## Write consistency (clusters only), can be: "any", "one", "quorum", "all"
    # write_consistency = "any"

    ## Timeout for HTTP messages.
    # timeout = "5s"

    ## HTTP Basic Auth
    username = "telegraf"
    password = "telegraf"

    ## HTTP User-Agent
    # user_agent = "telegraf"

    ## UDP payload size is the maximum packet size to send.
    # udp_payload = 512

    ## Optional SSL Config
    # ssl_ca = "/etc/telegraf/ca.pem"
    # ssl_cert = "/etc/telegraf/cert.pem"
    # ssl_key = "/etc/telegraf/key.pem"
    ## Use SSL but skip chain & host verification
    # insecure_skip_verify = false

    ## HTTP Proxy override, if unset values the standard proxy environment
    ## variables are consulted to determine which proxy, if any, should be used.
    # http_proxy = "http://corporate.proxy:3128"

    ## Additional HTTP headers
    # http_headers = {"X-Special-Header" = "Special-Value"}

    ## HTTP Content-Encoding for write request body, can be set to "gzip" to
    ## compress body or "identity" to apply no encoding.
    # content_encoding = "identity"

    ## When true, Telegraf will output unsigned integers as unsigned values,
    ## i.e.: "42u".  You will need a version of InfluxDB supporting unsigned
    ## integer values.  Enabling this option will result in field type errors if
    ## existing data has been written.
    # influx_uint_support = false


    ###############################################################################
    #                            INPUT PLUGINS                                    #
    ###############################################################################

    # Read metrics about cpu usage
    [[inputs.cpu]]
      ## Whether to report per-cpu stats or not
      percpu = true
      ## Whether to report total system cpu stats or not
      totalcpu = true
      ## If true, collect raw CPU time metrics.
      collect_cpu_time = false
      ## If true, compute and report the sum of all non-idle CPU states.
      report_active = false


    # Read metrics about disk usage by mount point
    [[inputs.disk]]
      ## By default stats will be gathered for all mount points.
      ## Set mount_points will restrict the stats to only the specified mount points.
      # mount_points = ["/"]

      ## Ignore mount points by filesystem type.
      ignore_fs = ["tmpfs", "devtmpfs", "devfs"]


    # Read metrics about disk IO by device
    [[inputs.diskio]]
      ## By default, telegraf will gather stats for all devices including
      ## disk partitions.
      ## Setting devices will restrict the stats to the specified devices.
      # devices = ["sda", "sdb", "vd*"]
      ## Uncomment the following line if you need disk serial numbers.
      # skip_serial_number = false
      #
      ## On systems which support it, device metadata can be added in the form of
      ## tags.
      ## Currently only Linux is supported via udev properties. You can view
      ## available properties for a device by running:
      ## 'udevadm info -q property -n /dev/sda'
      # device_tags = ["ID_FS_TYPE", "ID_FS_USAGE"]
      #
      ## Using the same metadata source as device_tags, you can also customize the
      ## name of the device via templates.
      ## The 'name_templates' parameter is a list of templates to try and apply to
      ## the device. The template may contain variables in the form of '$PROPERTY' or
      ## '${PROPERTY}'. The first template which does not contain any variables not
      ## present for the device is used as the device name tag.
      ## The typical use case is for LVM volumes, to get the VG/LV name instead of
      ## the near-meaningless DM-0 name.
      # name_templates = ["$ID_FS_LABEL","$DM_VG_NAME/$DM_LV_NAME"]


    # Get kernel statistics from /proc/stat
    [[inputs.kernel]]
      # no configuration


    # Read metrics about memory usage
    [[inputs.mem]]
      # no configuration


    # Get the number of processes and group them by status
    [[inputs.processes]]
      # no configuration


    # Read metrics about swap memory usage
    [[inputs.swap]]
      # no configuration


    # Read metrics about system load & uptime
    [[inputs.system]]
      # no configuration

      # # Read specific statistics per cgroup
      [[inputs.cgroup]]
      #   ## Directories in which to look for files, globs are supported.
      #   ## Consider restricting paths to the set of cgroups you really
      #   ## want to monitor if you have a large number of cgroups, to avoid
      #   ## any cardinality issues.
      #   # paths = [
      #   #   "/cgroup/memory",
      #   #   "/cgroup/memory/child1",
      #   #   "/cgroup/memory/child2/*",
      #   # ]
      #   ## cgroup stat fields, as file names, globs are supported.
      #   ## these file names are appended to each path from above.
      files = ["memory.*usage*", "memory.limit_in_bytes"]

      # # Read metrics about docker containers
       [[inputs.docker]]
      #   ## Docker Endpoint
      #   ##   To use TCP, set endpoint = "tcp://[ip]:[port]"
      #   ##   To use environment variables (ie, docker-machine), set endpoint = "ENV"
         endpoint = "unix:///var/run/docker.sock"
      #
      #   ## Set to true to collect Swarm metrics(desired_replicas, running_replicas)
      #   gather_services = false
      #
      #   ## Only collect metrics for these containers, collect all if empty
         container_names = []
      #
      #   ## Containers to include and exclude. Globs accepted.
      #   ## Note that an empty array for both will include all containers
      #   container_name_include = []
      #   container_name_exclude = []
      #
      #   ## Container states to include and exclude. Globs accepted.
      #   ## When empty only containers in the "running" state will be captured.
      #   # container_state_include = []
      #   # container_state_exclude = []
      #
      #   ## Timeout for docker list, info, and stats commands
      #   timeout = "5s"
      #
      #   ## Whether to report for each container per-device blkio (8:0, 8:1...) and
      #   ## network (eth0, eth1, ...) stats or not
      #   perdevice = true
      #   ## Whether to report for each container total blkio and network stats or not
      #   total = false
      #   ## Which environment variables should we use as a tag
      #   ##tag_env = ["JAVA_HOME", "HEAP_SIZE"]
      #
      #   ## docker labels to include and exclude as tags.  Globs accepted.
      #   ## Note that an empty array for both will include all labels as tags
      #   docker_label_include = []
      #   docker_label_exclude = []
      #
      #   ## Optional SSL Config
      #   # ssl_ca = "/etc/telegraf/ca.pem"
      #   # ssl_cert = "/etc/telegraf/cert.pem"
      #   # ssl_key = "/etc/telegraf/key.pem"
      #   ## Use SSL but skip chain & host verification
      #   # insecure_skip_verify = false

      # # Read metrics about network interface usage
       [[inputs.net]]
      #   ## By default, telegraf gathers stats from any up interface (excluding loopback)
      #   ## Setting interfaces will tell it to gather these explicit interfaces,
      #   ## regardless of status.
      #   ##
      #    interfaces = ["eth0"]
      #   ##
      #   ## On linux systems telegraf also collects protocol stats.
      #   ## Setting ignore_protocol_stats to true will skip reporting of protocol metrics.
      #   ##
      #   # ignore_protocol_stats = false
      #   ##
      # # Read metrics from one or many mysql servers
       [[inputs.mysql]]
      #   ## specify servers via a url matching:
      #   ##  [username[:password]@][protocol[(address)]]/[?tls=[true|false|skip-verify|custom]]
      #   ##  see https://github.com/go-sql-driver/mysql#dsn-data-source-name
      #   ##  e.g.
      #   ##    servers = ["user:passwd@tcp(127.0.0.1:3306)/?tls=false"]
      #   ##    servers = ["user@tcp(127.0.0.1:3306)/?tls=false"]
      #   #
      #   ## If no servers are specified, then localhost is used as the host.
         servers = ["tcp(127.0.0.1:3306)/"]
      #
      #   ## Selects the metric output format.
      #   ##
      #   ## This option exists to maintain backwards compatibility, if you have
      #   ## existing metrics do not set or change this value until you are ready to
      #   ## migrate to the new format.
      #   ##
      #   ## If you do not have existing metrics from this plugin set to the latest
      #   ## version.
      #   ##
      #   ## Telegraf >=1.6: metric_version = 2
      #   ##           <1.6: metric_version = 1 (or unset)
      #   metric_version = 2
      #
      #   ## the limits for metrics form perf_events_statements
      #   perf_events_statements_digest_text_limit  = 120
      #   perf_events_statements_limit              = 250
      #   perf_events_statements_time_limit         = 86400
      #   #
      #   ## if the list is empty, then metrics are gathered from all databasee tables
      #   table_schema_databases                    = []
      #   #
      #   ## gather metrics from INFORMATION_SCHEMA.TABLES for databases provided above list
      #   gather_table_schema                       = false
      #   #
      #   ## gather thread state counts from INFORMATION_SCHEMA.PROCESSLIST
         gather_process_list                       = true
      #   #
      #   ## gather thread state counts from INFORMATION_SCHEMA.USER_STATISTICS
         gather_user_statistics                    = true
         #   gather_info_schema_auto_inc               = true
         #   #
         #   ## gather metrics from INFORMATION_SCHEMA.INNODB_METRICS
            gather_innodb_metrics                     = true
         #   #
         #   ## gather metrics from SHOW SLAVE STATUS command output
         #   gather_slave_status                       = true
         #   #
         #   ## gather metrics from SHOW BINARY LOGS command output
            gather_binary_logs                        = false
         #   #
         #   ## gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_TABLE
         #   gather_table_io_waits                     = false
         #   #
         #   ## gather metrics from PERFORMANCE_SCHEMA.TABLE_LOCK_WAITS
         #   gather_table_lock_waits                   = false
         #   #
         #   ## gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_INDEX_USAGE
         #   gather_index_io_waits                     = false
         #   #
         #   ## gather metrics from PERFORMANCE_SCHEMA.EVENT_WAITS
         #   gather_event_waits                        = false
         #   #
         #   ## gather metrics from PERFORMANCE_SCHEMA.FILE_SUMMARY_BY_EVENT_NAME
         #   gather_file_events_stats                  = false
         #   #
         #   ## gather metrics from PERFORMANCE_SCHEMA.EVENTS_STATEMENTS_SUMMARY_BY_DIGEST
         #   gather_perf_events_statements             = false
         #   #
         #   ## Some queries we may want to run less often (such as SHOW GLOBAL VARIABLES)
         #   interval_slow                   = "30m"
         #
         #   ## Optional SSL Config (will be used if tls=custom parameter specified in server uri)
         #   ssl_ca = "/etc/telegraf/ca.pem"
         #   ssl_cert = "/etc/telegraf/cert.pem"
         #   ssl_key = "/etc/telegraf/key.pem"
         # # Read metrics from one or many postgresql servers
          [[inputs.postgresql]]
         #   ## specify address via a url matching:
         #   ##   postgres://[pqgotest[:password]]@localhost[/dbname]\
         #   ##       ?sslmode=[disable|verify-ca|verify-full]
         #   ## or a simple string:
         #   ##   host=localhost user=pqotest password=... sslmode=... dbname=app_production
         #   ##
         #   ## All connection parameters are optional.
         #   ##
         #   ## Without the dbname parameter, the driver will default to a database
         #   ## with the same name as the user. This dbname is just for instantiating a
         #   ## connection with the server and doesn't restrict the databases we are trying
         #   ## to grab metrics for.
         #   ##
            address = "postgres://postgres:123456@localhost:5432"
         #   ## A custom name for the database that will be used as the "server" tag in the
         #   ## measurement output. If not specified, a default one generated from
         #   ## the connection address is used.
         #   # outputaddress = "db01"
         #
         #   ## connection configuration.
         #   ## maxlifetime - specify the maximum lifetime of a connection.
         #   ## default is forever (0s)
         #   max_lifetime = "0s"
         #
         #   ## A  list of databases to explicitly ignore.  If not specified, metrics for all
         #   ## databases are gathered.  Do NOT use with the 'databases' option.
         ignored_databases = [ "template0", "template1"]
         #
         #   ## A list of databases to pull metrics about. If not specified, metrics for all
         #   ## databases are gathered.  Do NOT use with the 'ignored_databases' option.
         #   # databases = ["app_production", "testing"]
         # # Read metrics from one or many postgresql servers
          [[inputs.postgresql_extensible]]
         #   ## specify address via a url matching:
         #   ##   postgres://[pqgotest[:password]]@localhost[/dbname]\
         #   ##       ?sslmode=[disable|verify-ca|verify-full]
         #   ## or a simple string:
         #   ##   host=localhost user=pqotest password=... sslmode=... dbname=app_production
         #   #
         #   ## All connection parameters are optional.  #
         #   ## Without the dbname parameter, the driver will default to a database
         #   ## with the same name as the user. This dbname is just for instantiating a
         #   ## connection with the server and doesn't restrict the databases we are trying
         #   ## to grab metrics for.
         #   #
          address = "postgres://postgres:123456@localhost:5432"
         #
         #   ## connection configuration.
         #   ## maxlifetime - specify the maximum lifetime of a connection.
         #   ## default is forever (0s)
         #   max_lifetime = "0s"
         #
         #   ## A list of databases to pull metrics about. If not specified, metrics for all
         #   ## databases are gathered.
         #   ## databases = ["app_production", "testing"]
         #   #
         #   ## A custom name for the database that will be used as the "server" tag in the
         #   ## measurement output. If not specified, a default one generated from
         #   ## the connection address is used.
         #   # outputaddress = "db01"
         #   #
         #   ## Define the toml config where the sql queries are stored
         #   ## New queries can be added, if the withdbname is set to true and there is no
         #   ## databases defined in the 'databases field', the sql query is ended by a
         #   ## 'is not null' in order to make the query succeed.
         #   ## Example :
         #   ## The sqlquery : "SELECT * FROM pg_stat_database where datname" become
         #   ## "SELECT * FROM pg_stat_database where datname IN ('postgres', 'pgbench')"
         #   ## because the databases variable was set to ['postgres', 'pgbench' ] and the
         #   ## withdbname was true. Be careful that if the withdbname is set to false you
         #   ## don't have to define the where clause (aka with the dbname) the tagvalue
         #   ## field is used to define custom tags (separated by commas)
         #   ## The optional "measurement" value can be used to override the default
         #   ## output measurement name ("postgresql").
         #   #
         #   ## Structure :
         #   ## [[inputs.postgresql_extensible.query]]
         #   ##   sqlquery string
         #   ##   version string
         #   ##   withdbname boolean
         #   ##   tagvalue string (comma separated)
         #   ##   measurement string
         #   [[inputs.postgresql_extensible.query]]
         #     sqlquery="SELECT * FROM pg_stat_database"
         #     version=901
         #     withdbname=false
         #     tagvalue=""
         #     measurement=""
         #   [[inputs.postgresql_extensible.query]]
         #     sqlquery="SELECT * FROM pg_stat_bgwriter"
         #     version=901
         #     withdbname=false
         #     tagvalue="postgresql.stats"
         
            databases = ["infraV3", "postgres"]
         [[inputs.postgresql_extensible.query]] sqlquery="select count(1) as amount, state from pg_stat_activity where usename != 'telegraf' group by state" version=901 withdbname=false tagvalue="state" measurement="pg_connections"
         [[inputs.postgresql_extensible.query]] sqlquery="SELECT schema_name, pg_size_pretty(sum(table_size)::bigint) as disk_space, ((sum(table_size)::bigint) / 1024) / 1024 as size_MB FROM (SELECT pg_catalog.pg_namespace.nspname as schema_name, pg_relation_size(pg_catalog.pg_class.oid) as table_size FROM pg_catalog.pg_class JOIN pg_catalog.pg_namespace ON relnamespace = pg_catalog.pg_namespace.oid) t WHERE t.schema_name not in ('information_schema','pg_catalog','pg_toast') GROUP BY schema_name ORDER BY schema_name" version=901 withdbname=false tagvalue="schema_name" measurement="pg_schemas_size"
         [[inputs.postgresql_extensible.query]] sqlquery="select pg_xlog_location_diff(sent_location, write_location)::integer as write_lag, pg_xlog_location_diff(sent_location, flush_location)::integer as flush_lag, pg_xlog_location_diff(sent_location, replay_location)::integer as replay_lag from pg_stat_replication;" version=901 withdbname=false tagvalue="" measurement="pg_stat_replication"
